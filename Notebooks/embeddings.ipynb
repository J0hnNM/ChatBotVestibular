{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FdE7Q8MsyxuW"
      ],
      "authorship_tag": "ABX9TyMzF2zihXi85DCQQS2ne4Zo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/merovivant/AssistenteVestibular/blob/main/Notebooks/embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings\n",
        "Criando os embeddings de texto e armazenando os vetores criados na nuvem do Pinecone"
      ],
      "metadata": {
        "id": "4LB7lDLHadGS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importações"
      ],
      "metadata": {
        "id": "byrE8b7Rakc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install openai langchain pinecone-client tiktoken\n",
        "import os\n",
        "import getpass\n",
        "import pinecone\n",
        "from google.colab import drive\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
        "\n",
        "drive.mount('/drive')\n",
        "file_path = os.path.join('/drive/MyDrive/Chatbot/texto.md')"
      ],
      "metadata": {
        "id": "eK7jp1zSUa9F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35a48604-3ea2-4b63-e8f3-5e58b331bfcc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformações de Texto"
      ],
      "metadata": {
        "id": "FdE7Q8MsyxuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Abrindo o arquivo markdown do drive e transformando em string\n",
        "with open(file_path, \"r\") as f:\n",
        "  texto = f.read()"
      ],
      "metadata": {
        "id": "3Olz80M-gtkN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headers_to_split_on = [\n",
        "    (\"#\", \"Header 1\"),\n",
        "    (\"##\", \"Header 2\"),\n",
        "    (\"###\", \"Header 3\"),\n",
        "    (\"####\", \"Header 4\"),\n",
        "]\n",
        "\n",
        "#Dividindo o texto no nível das tags markdown\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "md_header_splits = markdown_splitter.split_text(texto)\n",
        "\n",
        "#Dividindo o texto no nível de caracteres\n",
        "chunk_size = 400\n",
        "chunk_overlap = 100\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
        ")\n",
        "\n",
        "# Split\n",
        "splits = text_splitter.split_documents(md_header_splits)"
      ],
      "metadata": {
        "id": "g9vzBnDbgxwZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Código"
      ],
      "metadata": {
        "id": "j9Ria5K6y1TR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando as chaves de API necessárias\n",
        "os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")\n",
        "os.environ[\"PINECONE_ENV\"] = getpass.getpass(\"Pinecone Environment:\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUi6sNgLP0jA",
        "outputId": "23667a1d-878e-47e7-d58c-4df8058ae1f0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pinecone API Key:··········\n",
            "Pinecone Environment:··········\n",
            "OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))"
      ],
      "metadata": {
        "id": "wgrfYt3qlS21"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Inicializando o Pinecone\n",
        "pinecone.init(\n",
        "    api_key=os.getenv(\"PINECONE_API_KEY\"),\n",
        "    environment=os.getenv(\"PINECONE_ENV\"),\n",
        ")\n",
        "\n",
        "index_name = \"unicampresolucao\"\n",
        "\n",
        "docsearch = Pinecone.from_documents(splits, embeddings, index_name=index_name)"
      ],
      "metadata": {
        "id": "zrO3SMlAQsvh"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}